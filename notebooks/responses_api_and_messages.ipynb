{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3790f38-26b5-469e-9a77-2cb069a569df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/gilbertharo/Documents/GitHub/ai-prompt-bootcamp/venv/lib/python3.9/site-packages (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e28d276-9d6e-4052-b789-8616de821116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken openai --quiet --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8a4d9-224e-4b8f-b91b-9b533fc4d350",
   "metadata": {},
   "source": [
    "# Managing Conversation State with OpenAI's API\n",
    "In this notebook, we'll walk through how to manage conversation state when interacting with the OpenAI API manually. We'll cover two approaches:\n",
    "\n",
    "### Manually managing conversation state: \n",
    "Building a conversation history list and appending messages for each turn.\n",
    "\n",
    "### Chaining responses using previous_response_id: \n",
    "This method enables the model to access prior context automatically.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "## 1. Manual Conversation State Management\n",
    "For this first example, we'll create a simple knock-knock joke conversation by manually constructing a conversation history. Each turn of the conversation is stored in a list of messages, where each message is a dictionary containing the role (user or assistant) and its content.\n",
    "\n",
    "This approach to state management is explicit and works well when you need fine-grained control over the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1e40bd-1112-4070-bc7f-910befd530fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1d8273-4d03-42bb-83f1-3a35decd6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e74e851-3811-4985-91dc-fcc11d54c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e23c17ad-1932-4551-be98-7dbe7011a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant response: Orange who?\n"
     ]
    }
   ],
   "source": [
    "# Construct a conversation history manually for a knock-knock joke\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"knock knock.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Orange.\"}\n",
    "]\n",
    "\n",
    "# Create a response using the conversation history\n",
    "response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=history\n",
    ")\n",
    "\n",
    "print(\"Assistant response:\", response.output_text)\n",
    "\n",
    "# (Optional) Inspect the full response object for further details\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03c9b3-fae8-47aa-84d6-abb356a5e2e2",
   "metadata": {},
   "source": [
    "## 2. Updating Conversation History Across Rounds\n",
    "\n",
    "In this example, we'll start a conversation with a simple joke prompt. We'll then update the conversation history with the model's response and ask for another joke. Updating the conversation history manually ensures the model has a full context of previous turns.\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd2fe2ec-4656-4c9c-a3d2-a4f35afed0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke: Sure! Here's a joke for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "Another joke: Of course! Here's another one:\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "# Start a new conversation with an initial joke request\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke\"}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=history,\n",
    "    store=False  # We're not storing the conversation automatically\n",
    ")\n",
    "\n",
    "print(\"Joke:\", response.output_text)\n",
    "\n",
    "# Update the conversation history with all parts of the response\n",
    "history += [{\"role\": el.role, \"content\": el.content} for el in response.output]\n",
    "\n",
    "# Ask for another joke in the same conversation\n",
    "history.append({\"role\": \"user\", \"content\": \"tell me another\"})\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=history,\n",
    "    store=False\n",
    ")\n",
    "\n",
    "print(\"Another joke:\", second_response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0d677-c178-40a9-95df-d94da5848a3c",
   "metadata": {},
   "source": [
    "## 3. Chaining Responses with previous_response_id\n",
    "\n",
    "Another way to carry conversation context is to chain responses by using the previous_response_id parameter. With this method, you initiate a conversation and then provide additional requests that reference the previous response.\n",
    "\n",
    "Below is an example where we first ask the model for a joke and then ask it to explain why that joke is funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1598370c-d1a1-4aa7-a319-877009fc2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke: Sure! Here's a joke for you:\n",
      "\n",
      "Why don't skeletons fight each other?\n",
      "\n",
      "Because they don't have the guts!\n",
      "Explanation: Absolutely! This joke is funny because it uses a play on words involving a double meaning of the phrase \"don't have the guts.\"\n",
      "\n",
      "1. **Literal meaning:** Skeletons literally don't have gutsâ€”they are just bones without any internal organs like intestines or stomach.\n",
      "\n",
      "2. **Figurative meaning:** The phrase \"don't have the guts\" is an idiom meaning \"don't have the courage\" to do something.\n",
      "\n",
      "So, the joke plays on the fact that skeletons both physically lack guts and, humorously, would therefore lack the courage to fight. The surprise comes from linking the literal anatomical fact with the figurative expression in a clever way. This clever wordplay creates the humor.\n"
     ]
    }
   ],
   "source": [
    "# Ask for a joke\n",
    "response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=\"tell me a joke\"\n",
    ")\n",
    "\n",
    "print(\"Joke:\", response.output_text)\n",
    "\n",
    "# Chain the next request using previous_response_id so the model retains context\n",
    "second_response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"user\", \"content\": \"explain why this is funny.\"}]\n",
    ")\n",
    "\n",
    "print(\"Explanation:\", second_response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a876e-748a-41c3-9395-d23a07423c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-prompt-bootcamp)",
   "language": "python",
   "name": "ai-prompt-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
